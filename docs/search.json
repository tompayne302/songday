[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Song day 2023 prediction model",
    "section": "",
    "text": "At the beginning of the film “The Social Network”, we are made aware that Eduardo Saverin (played by Andrew Garfield) had made $300,000 in one summer by trading oil futures. Mark Zuckerberg (played by Jesse Eisenberg), somewhat tongue-in-cheek, explains to Erica Albright (played by Rooney Mara) that Eduardo was able to do this because “He likes meteorology.” Albright, confused, responds, “You said it was oil futures?” Zuckerberg, triumphant that his (soon to be ex-) girlfriend has walked into his trap, explains: “You can read the weather, you can predict the price of heating oil!”\nLike Eduardo, we like making predictions. However, we aren’t smart enough to use them to make money, so instead, we will use them to make Pollard do more shots of fireball (admittedly a relatively straightforward task).\nShout out to Sean for collecting all these data. Unfortunately we only have data from Song Days 2 and 3, but it’s enough to work with.\nBut before we dive in, we need to give out 4 awards based on cumulative evidence from the last 2 song days.\n\n1 Award: Overall best performer\nLet’s see who has accumulated the most votes over the 2 years. Devillee, Niko, Dee, Steph, and Braith submitted songs in Song day 2 but not 3. Soony, Bods, Eleanor, Seiya, and Holby submitted songs for Song day 3 but not 2. So, omitted these people.\nBelow in Table 1 I show the ranked for total cumulative votes.\n\n\n\n\n\n\nTable 1:  Cumulative votes for all songs across Song days 2 and 3, excluding\npeople who didn’t attend one of those years. \n  \n    \n    \n      \n      Total votes\n      Rank\n    \n  \n  \n    Omar\n539\n1\n    Sean\n474\n2\n    Bonic\n438\n3\n    Lex\n414\n4\n    Wilson\n409\n5\n    Mitch\n345\n6\n    Jonny\n310\n7\n    Em\n292\n8\n    Michael\n242\n9\n    Glen\n235\n10\n    Veronica\n230\n11\n    Tish\n227\n12\n    Erana\n219\n13\n    Anna\n214\n14\n    Pollard\n199\n15\n    Oli\n193\n16\n    Claudia\n188\n17\n    Tom Payne\n184\n18\n    Josh\n178\n19\n    Anushka\n167\n20\n    Bibby\n134\n21\n    Liam\n109\n22\n    Zhang\n76\n23\n    Warmy\n36\n24\n  \n  \n  \n\n\n\n\n\n\n\n2 Award: Best performer on average\nBut that’s not the whole story, is it. We’re not just interested in the total number of votes someone got, we’re interested also in the average votes. Because the mean will be a highly skewed metric in our data, let’s use the median (middle) number of votes.\n\n\n\n\n\n\nTable 2:  Median votes for each song across Song days 2 and 3, excluding people\nwho didn’t attend one of those years. \n  \n    \n    \n      \n      Median number of votes\n      Rank\n    \n  \n  \n    Sean\n20.0\n1.0\n    Lex\n17.5\n2.5\n    Wilson\n17.5\n2.5\n    Bonic\n14.0\n5.0\n    Em\n14.0\n5.0\n    Michael\n14.0\n5.0\n    Mitch\n13.5\n7.0\n    Jonny\n11.5\n8.0\n    Anna\n11.0\n9.0\n    Tish\n8.5\n10.0\n    Erana\n8.0\n11.5\n    Omar\n8.0\n11.5\n    Claudia\n6.0\n13.5\n    Josh\n6.0\n13.5\n    Oli\n4.0\n15.0\n    Bibby\n3.5\n16.0\n    Pollard\n2.0\n17.5\n    Veronica\n2.0\n17.5\n    Tom Payne\n1.5\n19.0\n    Glen\n1.0\n20.0\n    Anushka\n0.0\n22.5\n    Liam\n0.0\n22.5\n    Warmy\n0.0\n22.5\n    Zhang\n0.0\n22.5\n  \n  \n  \n\n\n\n\n\n\n\n3 Award: Most consistent performer\nAnd the most consistent performer, indicated by the percentage of songs which got &gt;0 votes in Table 3:\n\n\n\n\n\n\nTable 3:  Percentage of songs with more than 0 votes across Song days 2 and 3,\nexcluding people who didn’t attend one of those years. \n  \n    \n    \n      \n      % of songs of 0 votes\n      Rank\n    \n  \n  \n    Sean\n0\n1.0\n    Wilson\n5\n2.0\n    Bonic\n10\n3.5\n    Lex\n10\n3.5\n    Mitch\n15\n5.0\n    Anna\n19\n6.0\n    Em\n20\n7.5\n    Tish\n20\n7.5\n    Erana\n21\n9.0\n    Josh\n26\n10.0\n    Omar\n30\n11.0\n    Claudia\n35\n12.5\n    Jonny\n35\n12.5\n    Bibby\n40\n15.5\n    Michael\n40\n15.5\n    Oli\n40\n15.5\n    Veronica\n40\n15.5\n    Glen\n43\n18.0\n    Pollard\n45\n19.0\n    Tom Payne\n50\n20.0\n    Anushka\n60\n21.5\n    Liam\n60\n21.5\n    Zhang\n74\n23.0\n    Warmy\n80\n24.0\n  \n  \n  \n\n\n\n\n\nNow to the predictions!\nThis is going to get very stats-heavy very quickly. If you don’t usually get a insatiable thirst for statistics arguments after 1.5 beers then it’s probably not worth reading on.\nThat being said, originally we had planned to experiment with various types of machine learning to create the ‘optimal’ prediction model based on our data, but alas we overestimated the amount of free time we would have to invest in this.\nSo, we will just thrust all our available variables into a (hopefully well-fitting) statistical model, and then use it to predict the votes for the 2023 songs. Essentially we are hitting a complex, nuanced, precise statistical question on the head with a sledgehammer.\nOur approach has 3 components:\n\nExtract all of the available data from Spotify for all of the songs from previous Song days, and combine this with the number of votes each song received.\nDesign a statistical model that best fits the data we have obtained from Spotify.\nPass the 2023 Song day data to the model, and make predictions about the number of votes each song will get.\n\n\n\n4 Extract data from Spotify\nThe approach we used here has been well-described elsewhere, for example, here and here.\nThe available metrics of interest to us that you can extract from Spotify are: popularity, acousticness, danceability, energy, instrumentalness, liveness, loudness, speechiness, tempo, valence, and key.\nNow what we are interested in is using a combination of these parameters to estimate the number of votes each song got.\n\n\n5 Design a statistical model\nSo, our outcome of interest is vote count. This is our ‘response’ or ‘dependent’ or ‘outcome’ variable (all synonyms).\nFirst let’s start by looking at how the votes have been distributed across songs over the years. As you can see, most songs get 0 votes.\n\n\n\n\n\nNow we need to find a statistical model that will fit this distribution. It’s hugely skewed count data, and has lots of 0’s, so traditional stuff won’t work.\nWhen we talk about ‘count data’ - you generally have two options: A Poisson-family model or negative binomial-family model, which essentially just differ by the estimation of a dispersion parameter in the negative binomial model. Both of these models can be zero-inflated if you need (to account for excess 0’s).\nWe’ll include all the covariables we have available and also an offset term for the year in which the vote was cast (as the max number of votes in 2021 was 10 whereas in 2022 it was 15). We’ll use the default priors in brms which is super bad practice but I CBF to think about this.\nWe are going to use a Bayesian approach, because it’s cooler and we are nothing if not really cool. We will use the brms package for regression. So, this is what our statistical model looks like in R (I’ve given the family as poisson but we will change this):\nformula &lt;- raw_votes ~ popularity + acousticness + danceability + energy + instrumentalness + \n                       liveness + loudness + speechiness + tempo + valence + key + duration + offset(log(year))\n\nmodel &lt;- brms::brm(formula, \n                   data = data, family = poisson(),\n                   iter = 4000,\n                   backend = \"cmdstanr\", \n                   cores = parallel::detectCores(),\n                   chains = 4,\n                   seed = 123)\nI’m going to start off with a zero-inflated negative binomial model because that’s the vibe I’m getting, but we will confirm this is the right choice.\nIn a Bayesian context, we can use leave-one-out cross validation (LOO) to compare the fit of statistical models - see this link for guidance.\nLet’s start by comparing a zero-inflated negative binomial model to a zero-inflated Poisson model. This method compares the LOO value for each model, and the better model is placed on top by default.\nAs shown below, the negative binomial provides a MUCH better fit.\n\n\n             elpd_diff se_diff\nzi_neg_binom     0.0       0.0\nzi_poisson   -1419.8     155.7\n\n\nBut is the zero-inflated component needed? Let’s check.\n\n\n            elpd_diff se_diff\nzi_negbinom   0.0       0.0  \nnegbinom    -74.4      11.2  \n\n\nThis suggests the zero-inflated model provides a much better fit.\nLet’s use another method to check that the zero-inflation is better. This method also compares model fit, using the LOO to assign weights. These are (heuristic) probabilities, with the values being the relative ‘weights’ we should assign to each model.\n\n\nzi_negbinom    negbinom \n 0.94895148  0.05104852 \n\n\nConsistent with the previous analysis, this suggests the zero-inflated model provides a much better fit suggesting we should assign 19x more weight to it. So we’ll go with that one.\nHowever, there are actually two ways of modelling excess zero-count data using the negative binomial distribution. We have used a zero-inflated or a hurdle negative binomial model. A description of the difference is here. Let’s see if there’s much of a difference in the fit of the models.\nThe answer is: not really, but the zero-inflated model is probably slightly better.\n\n\n                elpd_diff se_diff\nzi_negbinom      0.0       0.0   \nhurdle_negbinom -0.2       0.6   \n\n\nNow, using the DHARMa package, let’s look for evidence of nonlinearity in our data. Guidance on what this package does is here.\nFrom the plots in Figure 1, there are no issues with overdisperson/nonlinearity.\n\n\n\n\n\nFigure 1: Plots of simulated residuals from the DHARMa package in R.\n\n\n\n\nNow let’s actually have a look at the plots of the coefficients in Figure 2. Do any of these metrics actually individually predict the number of votes a song got?\nAnswer: not really.\n\n\n\n\n\nFigure 2: Plots of the posteriors of the Bayesian model coefficients.\n\n\n\n\nLet’s see how much of the variability in vote count is explained by our model. We’ll using the Bayesian version of \\(R^2\\):\n\n\n     Estimate  Est.Error       Q2.5     Q97.5\nR2 0.06735732 0.01618531 0.04092909 0.1037255\n\n\nSo it explains 6% of the variability. That’s a horrifically performing model.\nAlas, let’s press on.\n\n\n6 Run the predictions\nNow for the fun! We’ll use the marginaleffects package to calculate the predicted vote count for each of the new songs, based on their values for all the variables in the model. Below, as a teaser, I will provide the songs which ranked 40-50:\n\n\n\n\n\n\n  \n    \n    \n      vote_estimate\n      track\n      artist\n      rank\n    \n  \n  \n    18.29412\nYou Spin Me Round (Like a Record)\nDead Or Alive\n40\n    18.28493\nBad Romance\nLady Gaga\n41\n    18.22394\nDog Days Are Over\nFlorence + The Machine\n42\n    18.19308\nRockstar\nNickelback\n43\n    18.11609\nSomething So Strong\nCrowded House\n44\n    17.82452\nHeads Will Roll - A-Trak Remix Radio Edit\nYeah Yeah Yeahs, A-Trak\n45\n    17.73934\nSilver Springs - 2004 Remaster\nFleetwood Mac\n46\n    17.68636\nManeater\nNelly Furtado\n47\n    17.68585\nTell Me Why - Radio Edit\nSupermode, Axwell, Steve Angello\n48\n    17.64290\nTeenage Kicks\nThe Undertones\n49\n    17.55941\nBe My Baby\nThe Ronettes\n50"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]