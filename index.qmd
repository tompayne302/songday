---
title: "Song day 2023 prediction model"
author: 
  - name: Tom Payne & Will Armstrong
    affiliation: The Dawg House
date: today
---

```{r setup, include = FALSE}

library(httr)
library(magrittr)
library(rvest)
library(ggplot2)
library(spotifyr)
library(janitor)
library(dplyr)
library(fuzzyjoin)
library(brms)
library(marginaleffects)
library(rms)
library(coin)
library(pscl)
library(glmmTMB)
library(MASS)
library(performance)
library(AER)
library(patchwork)
library(ggeffects)
library(lmerTest)
library(gtsummary)
library(tidybayes)
library(mgcv)
library(gt)
library(DHARMa)

knitr::opts_chunk$set(echo = F, message = F, warning = F, error = T, 
                      fig.height = 3, out.width = "90%", 
                      dev = "png", dpi = 300, cache = T)

import_path_mac <- '/Users/thomaspayne/Documents/MPhil/Spotify/'
export_path_mac <- '/Users/thomaspayne/Documents/MPhil/Spotify/'

current_date <- as.Date("2023-09-01")

data <- read.csv(paste0(export_path_mac, "Final_all.csv")) %>%
  clean_names() %>%
  mutate(across(acousticness:valence, scale)) %>%
  mutate(duration = duration/(1000 * 60)) %>%
  mutate(
    release_date_converted = case_when(
      nchar(release_date) == 4 ~ as.Date(paste0(release_date, "-01-01")),
      TRUE ~ as.Date(release_date, format = "%Y-%m-%d")
    ),
    years_since_release = as.numeric(difftime(current_date, release_date_converted, units = "weeks") / 52.25)
  ) %>%
  dplyr::select(-release_date_converted) %>%
  mutate(across(acousticness:valence, as.numeric),
         year = as.numeric(ifelse(song_day == "2", 10, 15)))

new_data <- read.csv(paste0(export_path_mac, "2023-Songs.csv")) %>%
  clean_names() %>%
  mutate(across(acousticness:valence, scale)) %>%
  mutate(duration = duration/(1000 * 60),
         year = 15) %>%
  mutate(
    release_date_converted = case_when(
      nchar(release_date) == 4 ~ as.Date(paste0(release_date, "-01-01")),
      TRUE ~ as.Date(release_date, format = "%Y-%m-%d")
    ),
    years_since_release = as.numeric(difftime(current_date, release_date_converted, units = "weeks") / 52.25)
  ) %>%
  dplyr::select(-release_date_converted) %>%
  mutate(across(acousticness:valence, as.numeric))

formula <- raw_votes ~ popularity + acousticness + danceability + energy + instrumentalness + 
                       liveness + loudness + speechiness + tempo + valence + key + duration + offset(log(year))

model_brms_zi_nb <- brms::brm(formula, 
                              data = data, family = zero_inflated_negbinomial(),
                              iter = 4000,
                              backend = "cmdstanr", 
                              cores = parallel::detectCores(),
                              chains = 4,
                              seed = 123)

model_brms_hnb <- update(model_brms_zi_nb, family = hurdle_negbinomial())

model_brms_nb <- update(model_brms_zi_nb, family = negbinomial())

model_brms_poi <- update(model_brms_zi_nb, family = poisson())

model_brms_zi_poi <- update(model_brms_zi_nb, family = zero_inflated_poisson())

```

At the beginning of the film "The Social Network", we are made aware that Eduardo Saverin (played by Andrew Garfield) had made $300,000 in one summer by trading oil futures. Mark Zuckerberg (played by Jesse Eisenberg), somewhat tongue-in-cheek, explains to Erica Albright (played by Rooney Mara) that Eduardo was able to do this because "He likes meteorology." Albright, confused, responds, "You said it was oil futures?" Zuckerberg, triumphant that his (soon to be ex-) girlfriend has walked into his trap, explains: "You can read the weather, you can predict the price of heating oil!"

Like Eduardo, we like making predictions. However, we aren't smart enough to use them to make money, so instead, we will use them to make Pollard do more shots of fireball (admittedly a relatively straightforward task).

Shout out to Sean for collecting all these data. Unfortunately we only have data from Song Days 2 and 3, but it's enough to work with.

But before we dive in, we need to give out 4 awards based on cumulative evidence from the last 2 song days.

# Award: Overall best performer

Let's see who has accumulated the most votes over the 2 years. Devillee, Niko, Dee, Steph, and Braith submitted songs in Song day 2 but not 3. Soony, Bods, Eleanor, Seiya, and Holby submitted songs for Song day 3 but not 2. So, omitted these people.

Below in @tbl-total-votes I show the ranked for total cumulative votes. 

```{r}
#| label: tbl-total-votes
#| tbl-cap: Cumulative votes for all songs across Song days 2 and 3, excluding people who didn't attend one of those years.


names_to_exclude <- c("Devilee", "Nikolai", "Dee", "Steph", "Braith", "Soony", "Bods", "Eleanor", "Seiya", "Holby")

data_rank <- data %>%
  mutate(person = tolower(person),  # Convert 'person' to lower case
         person = ifelse(person == "tom", "tom payne", ifelse(person == "#n/a", NA, person)),
         person = tools::toTitleCase(as.character(person)),
         person = factor(person)) %>%
  filter(!is.na(person), !person %in% names_to_exclude) %>%
  group_by(person) %>%
  summarise(total_votes = sum(raw_votes, na.rm = TRUE)) %>%
  ungroup()
  
data_rank %>%
  mutate(rank = rank(-total_votes)) %>%
  arrange(rank) %>%
  gt(rowname_col = "person") %>%
  gt::cols_label(total_votes = "Total votes",
              rank = "Rank") %>%
  gt::tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(columns = everything()))
  
```

# Award: Best performer on average

But that's not the whole story, is it. We're not just interested in the *total* number of votes someone got, we're interested also in the *average* votes. Because the mean will be a highly skewed metric in our data, let's use the *median* (middle) number of votes. 

```{r}
#| label: tbl-medial-votes
#| tbl-cap: Median votes for each song across Song days 2 and 3, excluding people who didn't attend one of those years.

names_to_exclude <- c("Devilee", "Nikolai", "Dee", "Steph", "Braith", "Soony", "Bods", "Eleanor", "Seiya", "Holby")

data_rank <- data %>%
  mutate(person = tolower(person),  # Convert 'person' to lower case
         person = ifelse(person == "tom", "tom payne", ifelse(person == "#n/a", NA, person)),
         person = tools::toTitleCase(as.character(person)),
         person = factor(person)) %>%
  filter(!is.na(person), !person %in% names_to_exclude) %>%
  group_by(person) %>%
  summarise(avg_votes = median(raw_votes, na.rm = TRUE)) %>%
  ungroup()

data_rank %>%
  mutate(rank = rank(-avg_votes)) %>%
  arrange(rank) %>%
  gt(rowname_col = "person") %>%
  gt::cols_label(avg_votes = "Median number of votes",
              rank = "Rank") %>%
   gt::tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(columns = everything()))
```

# Award: Most consistent performer

And the most consistent performer, indicated by the percentage of songs which got >0 votes in @tbl-consistent-votes:

```{r}
#| label: tbl-consistent-votes
#| tbl-cap: Percentage of songs with more than 0 votes across Song days 2 and 3, excluding people who didn't attend one of those years.

names_to_exclude <- c("Devilee", "Nikolai", "Dee", "Steph", "Braith", "Soony", "Bods", "Eleanor", "Seiya", "Holby")

data_rank <- data %>%
  mutate(person = tolower(person),  # Convert 'person' to lower case
         person = ifelse(person == "tom", "tom payne", ifelse(person == "#n/a", NA, person)),
         person = tools::toTitleCase(as.character(person)),
         person = factor(person)) %>%
  filter(!is.na(person), !person %in% names_to_exclude) %>%
  group_by(person) %>%
  summarise(
    zero_votes_percent = round(sum(raw_votes == 0, na.rm = TRUE) / n() * 100, 0
  )) %>%
  mutate(
    rank = rank(zero_votes_percent)  # Negative sign for descending order
  ) %>%
  arrange(rank)

data_rank %>%
  gt(rowname_col = "person") %>%
  gt::cols_label(zero_votes_percent = "% of songs of 0 votes",
              rank = "Rank") %>%
   gt::tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(columns = everything()))
```

Now to the predictions!

This is going to get very stats-heavy very quickly. If you don't usually get a insatiable thirst for statistics arguments after 1.5 beers then it's probably not worth reading on.

That being said, originally we had planned to experiment with various types of machine learning to create the 'optimal' prediction model based on our data, but alas we overestimated the amount of free time we would have to invest in this. 

So, we will just thrust all our available variables into a (hopefully well-fitting) statistical model, and then use it to predict the votes for the 2023 songs. Essentially we are hitting a complex, nuanced, precise statistical question on the head with a sledgehammer.

Our approach has 3 components:

1. Extract all of the available data from Spotify for all of the songs from previous Song days, and combine this with the number of votes each song received.
2. Design a statistical model that best fits the data we have obtained from Spotify.
3. Pass the 2023 Song day data to the model, and make predictions about the number of votes each song will get.

# Extract data from Spotify

The approach we used here has been well-described elsewhere, for example, [here](https://martijnvanvreeden.nl/collecting-spotify-data-with-r/) and [here](https://mcmullarkey.github.io/mcm-blog/posts/2022-01-07-spotify-api-r/). 

The available metrics of interest to us that you can extract from Spotify are: popularity, acousticness, danceability, energy, instrumentalness, liveness, loudness, speechiness, tempo, valence, and key.

Now what we are interested in is using a combination of these parameters to estimate the number of votes each song got.

# Design a statistical model

So, our outcome of interest is vote count. This is our 'response' or 'dependent' or 'outcome' variable (all synonyms). 

First let's start by looking at how the votes have been distributed across songs over the years. As you can see, most songs get 0 votes.

```{r}
d <- data %>% 
  filter(!is.na(raw_votes))

ggplot(d, aes(x = raw_votes)) +
  geom_histogram(binwidth = 1,color = "blue") +
  labs(x = "Number of votes", y = "Count") +
  theme_light() 
```

Now we need to find a statistical model that will fit this distribution. It's hugely skewed count data, and has lots of 0's, so traditional stuff won't work.

When we talk about 'count data' - you generally have two options: A Poisson-family model or negative binomial-family model, which essentially just differ by the estimation of a dispersion parameter in the negative binomial model. Both of these models can be zero-inflated if you need (to account for excess 0's). 

We'll include all the covariables we have available and also an offset term for the year in which the vote was cast (as the max number of votes in 2021 was 10 whereas in 2022 it was 15). We'll use the default priors in `brms` which is super bad practice but I CBF to think about this.

We are going to use a Bayesian approach, because it's cooler and we are nothing if not really cool. We will use the `brms` package for regression. So, this is what our statistical model looks like in R (I've given the family as `poisson` but we will change this):

``` r
formula <- raw_votes ~ popularity + acousticness + danceability + energy + instrumentalness + 
                       liveness + loudness + speechiness + tempo + valence + key + duration + offset(log(year))

model <- brms::brm(formula, 
                   data = data, family = poisson(),
                   iter = 4000,
                   backend = "cmdstanr", 
                   cores = parallel::detectCores(),
                   chains = 4,
                   seed = 123)
```


I'm going to start off with a zero-inflated negative binomial model because that's the vibe I'm getting, but we will confirm this is the right choice.

In a Bayesian context, we can use leave-one-out cross validation (LOO) to compare the fit of statistical models - see [this link](https://bookdown.org/ajkurz/DBDA_recoded/model-comparison-and-hierarchical-modeling.html) for guidance.

Let's start by comparing a zero-inflated negative binomial model to a zero-inflated Poisson model. This method compares the LOO value for each model, and the better model is placed on top by default. 

As shown below, the negative binomial provides a MUCH better fit.

```{r}
zi_neg_binom <- add_criterion(model_brms_zi_nb, criterion = c("loo", "waic"))
zi_poisson <- add_criterion(model_brms_zi_poi, criterion = c("loo", "waic"))

loo_compare(zi_neg_binom, zi_poisson, criterion = "loo")

```

But is the zero-inflated component needed? Let's check.

```{r}
zi_negbinom <- add_criterion(model_brms_zi_nb, criterion = c("loo", "waic"))
negbinom <- add_criterion(model_brms_nb, criterion = c("loo", "waic"))

loo_compare(zi_negbinom, negbinom, criterion = "loo")
```

This suggests the zero-inflated model provides a much better fit.

Let's use another method to check that the zero-inflation is better. This method also compares model fit, using the LOO to assign weights. These are (heuristic) probabilities, with the values being the relative 'weights' we should assign to each model.

```{r}
model_weights(zi_negbinom, negbinom)
```

Consistent with the previous analysis, this suggests the zero-inflated model provides a much better fit suggesting we should assign 19x more weight to it. So we'll go with that one.

However, there are actually two ways of modelling excess zero-count data using the negative binomial distribution. We have used a zero-inflated or a hurdle negative binomial model. A description of the difference is [here](https://stats.stackexchange.com/questions/81457/what-is-the-difference-between-zero-inflated-and-hurdle-models). Let's see if there's much of a difference in the fit of the models.

The answer is: not really, but the zero-inflated model is probably slightly better.

```{r}
hurdle_negbinom <- add_criterion(model_brms_hnb, criterion = c("loo", "waic"))

loo_compare(zi_negbinom, hurdle_negbinom, criterion = "loo")
```

Now, using the DHARMa package, let's look for evidence of nonlinearity in our data. Guidance on what this package does is [here](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html). 

From the plots in @fig-dharma, there are no issues with overdisperson/nonlinearity.

```{r}
#| label: fig-dharma
#| fig-cap: Plots of simulated residuals from the DHARMa package in R.
#| fig-width: 14
#| fig-height: 6

model.check <- DHARMa::createDHARMa(
  simulatedResponse = t(posterior_predict(model_brms_zi_nb)),
  observedResponse = data$raw_votes,
  fittedPredictedResponse = apply(t(posterior_epred(model_brms_zi_nb)), 1, mean),
  integerResponse = TRUE)

plot(model.check)
title(main="Zero-inflated negative binomial model", line = 2, adj = 0)

```

Now let's actually have a look at the plots of the coefficients in @fig-coefs. Do any of these metrics actually individually predict the number of votes a song got?

Answer: not really.

```{r}
#| label: fig-coefs
#| fig-cap: Plots of the posteriors of the Bayesian model coefficients.

pivoted_draws_df <- model_brms_zi_nb %>%
  spread_draws(b_popularity, b_acousticness, b_danceability, b_energy, b_instrumentalness, 
               b_liveness, b_loudness, b_speechiness, b_tempo, b_valence, b_key, b_duration) %>%
  dplyr::select(-.chain, -.draw, -.iteration) %>%
  tidyr::pivot_longer(everything(), names_to = "variable", values_to = "draw")

ggplot(aes(x = exp(draw), y = variable, alpha = 0.9), 
       data = pivoted_draws_df) +
  geom_vline(xintercept = 1, color = "black", 
             size = 0.7) +
  stat_pointinterval() +
  theme_light() +
  scale_x_log10(breaks = c(0.75, 1, 1.5), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.7, 1.6), ylim=c(1,13)) +
  annotate("text", x = 0.8, y =12.5, label = "Decreased\nvotes") +
  annotate("text", x = 1.3, y = 12.5,  label = "Increased\nvotes") +
  labs(x="Incidence rate ratio") +
  ylab(NULL) +
  guides(alpha = "none")
```

Let's see how much of the variability in vote count is explained by our model. We'll using the Bayesian version of $R^2$:

```{r}
brms::bayes_R2(model_brms_zi_nb)
```

So it explains 6% of the variability. That's a horrifically performing model. 

Alas, let's press on. 

# Run the predictions

Now for the fun! We'll use the `marginaleffects` package to calculate the predicted vote count for each of the new songs, based on their values for all the variables in the model. Below, as a teaser, I will provide the songs which ranked 40-50:

```{r}
library(openxlsx)

df <- marginaleffects::predictions(model_brms_zi_nb, newdata = new_data) %>%
  arrange(-estimate) %>%
  mutate(rank = row_number()) %>%
  dplyr::select(estimate, track, artist, rank) %>%
  rename(vote_estimate = estimate)

openxlsx::write.xlsx(df, "predictions.xlsx")

df %>%
  filter(rank >= 40 & rank <= 50) %>%
  gt()

```

